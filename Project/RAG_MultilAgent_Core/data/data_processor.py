import streamlit as st
import pandas as pd
import json
from pymilvus import connections, Collection, utility
import time
from datetime import datetime, timedelta
import hashlib
import math


# ==================== ENHANCED CACHING ====================

# Connection caching v·ªõi retry logic
@st.cache_resource
def get_milvus_connection():
    """Cache Milvus connection - ch·ªâ t·∫°o 1 l·∫ßn"""
    try:
        connections.connect(
            alias="default",
            host="10.10.4.25",
            port="19530"
        )
        return True
    except Exception as e:
        st.error(f"‚ö† L·ªói k·∫øt n·ªëi Milvus: {e}")
        return False


def connect_to_milvus():
    """Wrapper function cho connection v·ªõi caching"""
    return get_milvus_connection()


# Collection data caching v·ªõi TTL d√†i h∆°n v√† batch loading - S·ª¨ D·ª§NG ID PAGINATION
@st.cache_data(
    ttl=7200,  # Cache 2 ti·∫øng thay v√¨ 50 ph√∫t
    max_entries=3,  # Gi·ªõi h·∫°n s·ªë cache entries
    show_spinner="üîÑ ƒêang t·∫£i d·ªØ li·ªáu t·ª´ Milvus..."
)
def load_collection_data():
    """Load d·ªØ li·ªáu t·ª´ collection v·ªõi caching t·ªëi ∆∞u v√† ID-based pagination"""
    try:
        collection_name = "product_collection_v4"

        if not utility.has_collection(collection_name):
            st.error(f"‚ö† Collection '{collection_name}' kh√¥ng t·ªìn t·∫°i!")
            return None

        collection = Collection(collection_name)
        collection.load()

        # Get total count ƒë·ªÉ hi·ªÉn th·ªã progress
        total_count = collection.num_entities
        batch_size = 16384  # Gi·ªØ nguy√™n batch size
        
        all_results = []
        last_id = ""  # B·∫Øt ƒë·∫ßu t·ª´ ID r·ªóng
        batch_idx = 0
        
        # Progress bar cho batch loading
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        while True:
            batch_idx += 1
            status_text.text(f"ƒêang t·∫£i batch {batch_idx} (t·ª´ ID: {last_id[:20]}...)...")
            
            try:
                # Query batch hi·ªán t·∫°i s·ª≠ d·ª•ng ID-based pagination
                if last_id:
                    # Query c√°c record c√≥ id_sanpham > last_id
                    expr = f'id_sanpham > "{last_id}"'
                else:
                    # Batch ƒë·∫ßu ti√™n - l·∫•y t·∫•t c·∫£
                    expr = ""
                
                batch_results = collection.query(
                    expr=expr,
                    output_fields=["id_sanpham", "platform", "description", "metadata", "date", "like", "comment", "share", "name_store"],
                    limit=batch_size,
                    # Kh√¥ng s·ª≠ d·ª•ng offset n·ªØa
                    consistency_level="Strong"
                )
                
                # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ th√¨ ƒë√£ load xong
                if not batch_results:
                    status_text.text("‚úÖ ƒê√£ load xong t·∫•t c·∫£ d·ªØ li·ªáu!")
                    break
                    
                # S·∫Øp x·∫øp batch results theo id_sanpham ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª±
                batch_results.sort(key=lambda x: x.get('id_sanpham', ''))
                
                all_results.extend(batch_results)
                
                # C·∫≠p nh·∫≠t last_id cho batch ti·∫øp theo
                last_id = batch_results[-1].get('id_sanpham', '')
                
                # Update progress d·ª±a tr√™n s·ªë records ƒë√£ load
                if total_count > 0:
                    progress = min(len(all_results) / total_count, 1.0)
                    progress_bar.progress(progress)
                
                # Small delay ƒë·ªÉ tr√°nh qu√° t·∫£i Milvus
                time.sleep(0.1)
                
                # Hi·ªÉn th·ªã th√¥ng tin ti·∫øn ƒë·ªô
                if batch_idx % 5 == 0:  # C·∫≠p nh·∫≠t th√¥ng tin m·ªói 5 batch
                    st.info(f"üìä ƒê√£ load: {len(all_results):,} / ~{total_count:,} records")
                    
            except Exception as batch_error:
                st.warning(f"L·ªói t·∫£i batch {batch_idx}: {batch_error}")
                # Th·ª≠ ti·∫øp t·ª•c v·ªõi batch ti·∫øp theo
                continue
        
        # Clear progress indicators
        progress_bar.empty()
        status_text.empty()
        
        if all_results:
            st.success(f"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng {len(all_results):,} records t·ª´ {batch_idx} batches")
        else:
            st.warning("‚ö† Kh√¥ng c√≥ d·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i")
            
        return all_results
        
    except Exception as e:
        st.error(f"‚ö† L·ªói load d·ªØ li·ªáu: {e}")
        return None


# Alternative batch loading v·ªõi pagination th√¥ng minh - S·ª¨ D·ª§NG ID PAGINATION
@st.cache_data(
    ttl=7200,
    max_entries=3,
    show_spinner="üîÑ ƒêang t·∫£i d·ªØ li·ªáu v·ªõi ID-based pagination..."
)
def load_collection_data_with_pagination():
    """Load d·ªØ li·ªáu v·ªõi ID-based pagination th√¥ng minh - kh√¥ng gi·ªõi h·∫°n offset"""
    try:
        collection_name = "product_collection_v4"

        if not utility.has_collection(collection_name):
            st.error(f"‚ö† Collection '{collection_name}' kh√¥ng t·ªìn t·∫°i!")
            return None

        collection = Collection(collection_name)
        collection.load()

        all_results = []
        batch_size = 16384
        last_id = ""  # B·∫Øt ƒë·∫ßu t·ª´ ID r·ªóng
        batch_count = 0
        
        # S·ª≠ d·ª•ng container cho progress tracking
        progress_container = st.container()
        
        while True:
            batch_count += 1
            
            try:
                # Query batch v·ªõi ID-based pagination
                if last_id:
                    # Query c√°c record c√≥ id_sanpham > last_id
                    expr = f'id_sanpham > "{last_id}"'
                else:
                    # Batch ƒë·∫ßu ti√™n
                    expr = ""
                
                batch_results = collection.query(
                    expr=expr,
                    output_fields=["id_sanpham", "platform", "description", "metadata", "date", "like", "comment", "share", "name_store"],
                    limit=batch_size,
                    consistency_level="Strong"
                )
                
                    
                # S·∫Øp x·∫øp batch results theo id_sanpham
                batch_results.sort(key=lambda x: x.get('id_sanpham', ''))
                
                all_results.extend(batch_results)
                
                # C·∫≠p nh·∫≠t last_id cho batch ti·∫øp theo
                last_id = batch_results[-1].get('id_sanpham', '')
            
                
                # Small delay ƒë·ªÉ tr√°nh qu√° t·∫£i
                time.sleep(0.05)
                
            except Exception as batch_error:
                # Th·ª≠ ti·∫øp t·ª•c thay v√¨ break
                if "timeout" not in str(batch_error).lower():
                    break  # Ch·ªâ break khi kh√¥ng ph·∫£i timeout
        
        # Clear progress container
        progress_container.empty()
        
        return all_results
        
    except Exception as e:
        st.error(f"‚ö† L·ªói load d·ªØ li·ªáu v·ªõi ID pagination: {e}")
        return None


# Cached collection status check
@st.cache_data(ttl=1800)  # Cache 30 ph√∫t
def check_collection_exists(collection_name):
    """Cache collection existence check"""
    try:
        return utility.has_collection(collection_name)
    except:
        return False


# Enhanced collection info v·ªõi ID-based estimation
@st.cache_data(ttl=900)  # Cache 15 ph√∫t
def get_collection_info():
    """L·∫•y th√¥ng tin collection v·ªõi ID-based batch estimation"""
    try:
        collection_name = "product_collection_v4"
        
        if not utility.has_collection(collection_name):
            return None
            
        collection = Collection(collection_name)
        collection.load()
        
        total_entities = collection.num_entities
        batch_size = 16384
        estimated_batches = math.ceil(total_entities / batch_size)
        
        # L·∫•y sample ID ƒë·ªÉ test query performance
        sample_query_time = 0
        try:
            start_time = time.time()
            sample_result = collection.query(
                expr="",
                output_fields=["id_sanpham"],
                limit=1
            )
            sample_query_time = time.time() - start_time
        except:
            sample_query_time = 0.1  # Default estimate
        
        return {
            'name': collection_name,
            'total_entities': total_entities,
            'batch_size': batch_size,
            'estimated_batches': estimated_batches,
            'estimated_load_time': estimated_batches * (sample_query_time + 0.1),  # Query time + processing
            'pagination_method': 'ID-based (unlimited)',
            'sample_query_time': sample_query_time
        }
        
    except Exception as e:
        st.error(f"L·ªói l·∫•y th√¥ng tin collection: {e}")
        return None


# ==================== OPTIMIZED DATA PARSING ====================

@st.cache_data(
    ttl=3600,  # Cache 1 ti·∫øng
    max_entries=5,
    show_spinner="üìä ƒêang x·ª≠ l√Ω metadata..."
)
def parse_metadata_cached(data_hash, data):
    """Parse metadata v·ªõi caching based on data hash"""
    return parse_metadata_internal(data)


def parse_metadata(data):
    """Parse metadata t·ª´ JSON v√† t·∫°o DataFrame v·ªõi caching"""
    if not data:
        return pd.DataFrame()

    # T·∫°o hash c·ªßa data ƒë·ªÉ cache
    data_str = json.dumps(data, sort_keys=True, default=str)  # default=str ƒë·ªÉ handle non-serializable objects
    data_hash = hashlib.md5(data_str.encode()).hexdigest()

    return parse_metadata_cached(data_hash, data)


def parse_metadata_internal(data):
    """Internal parsing function v·ªõi batch processing"""
    parsed_data = []
    
    # Batch processing ƒë·ªÉ t·ªëi ∆∞u memory
    batch_size = 1000
    total_items = len(data)
    
    for batch_start in range(0, total_items, batch_size):
        batch_end = min(batch_start + batch_size, total_items)
        batch_data = data[batch_start:batch_end]
        
        for item in batch_data:
            try:
                metadata = json.loads(item.get('metadata', '{}')) if isinstance(item.get('metadata'), str) else item.get('metadata', {})

                row = {
                    'id_sanpham': item.get('id_sanpham', ''),
                    'platform': item.get('platform', ''),
                    'description': item.get('description', ''),
                    'name_store': item.get('name_store', ''),
                    'date': item.get('date', ''),
                    'like': int(item.get('like', 0)) if str(item.get('like', 0)).isdigit() else 0,
                    'comment': int(item.get('comment', 0)) if str(item.get('comment', 0)).isdigit() else 0,
                    'share': int(item.get('share', 0)) if str(item.get('share', 0)).isdigit() else 0,
                }

                # Th√™m c√°c tr∆∞·ªùng metadata
                for key, value in metadata.items():
                    if isinstance(value, list):
                        row[key] = ', '.join(map(str, value))
                    else:
                        row[key] = str(value)

                parsed_data.append(row)
            except Exception as e:
                st.warning(f"L·ªói parse metadata cho item {item.get('id_sanpham', 'unknown')}: {e}")
                continue

    return pd.DataFrame(parsed_data)


# ==================== CACHED UTILITY FUNCTIONS ====================

@st.cache_data
def safe_int_convert(value):
    """Safely convert value to integer - cached"""
    try:
        if isinstance(value, (int, float)):
            return int(value)
        elif isinstance(value, str):
            # Remove commas and spaces, then convert
            clean_value = value.replace(',', '').replace(' ', '').strip()
            return int(clean_value) if clean_value.isdigit() else 0
        else:
            return 0
    except:
        return 0


@st.cache_data
def parse_engagement_string(engagement_str):
    """Parse engagement string v·ªõi caching"""
    if not engagement_str:
        return {"like": 0, "comment": 0, "share": 0}

    try:
        # Handle different possible formats
        engagement_dict = {"like": 0, "comment": 0, "share": 0}

        # Case 1: JSON-like string format
        if engagement_str.strip().startswith('{') and engagement_str.strip().endswith('}'):
            import json
            try:
                parsed = json.loads(engagement_str)
                if isinstance(parsed, dict):
                    return {
                        "like": safe_int_convert(parsed.get("like", 0)),
                        "comment": safe_int_convert(parsed.get("comment", 0)),
                        "share": safe_int_convert(parsed.get("share", 0))
                    }
            except:
                pass

        # Case 2: Dictionary-like string format (like, comment, share)
        if "like" in engagement_str.lower() or "comment" in engagement_str.lower() or "share" in engagement_str.lower():
            # Extract numbers after keywords
            import re

            like_match = re.search(r'like[\'\":\s]*(\d+)', engagement_str, re.IGNORECASE)
            comment_match = re.search(r'comment[\'\":\s]*(\d+)', engagement_str, re.IGNORECASE)
            share_match = re.search(r'share[\'\":\s]*(\d+)', engagement_str, re.IGNORECASE)

            if like_match:
                engagement_dict["like"] = int(like_match.group(1))
            if comment_match:
                engagement_dict["comment"] = int(comment_match.group(1))
            if share_match:
                engagement_dict["share"] = int(share_match.group(1))

            return engagement_dict

        # Case 3: Simple number format (fallback)
        clean_str = engagement_str.replace(',', '').replace(' ', '').strip()
        if clean_str.isdigit():
            total = int(clean_str)
            # Distribute as before for backward compatibility
            return {
                "like": int(total * 0.7),
                "comment": int(total * 0.2),
                "share": int(total * 0.1)
            }

        return engagement_dict

    except Exception as e:
        print(f"Error parsing engagement: {e}")
        return {"like": 0, "comment": 0, "share": 0}


# ==================== BATCH PROCESSING WITH CACHING ====================

@st.cache_data(
    ttl=1800,
    max_entries=10,
    show_spinner="‚ö° ƒêang x·ª≠ l√Ω batch data..."
)
def process_batch_data(data_batch_hash, data_batch):
    """Process batch data v·ªõi caching"""
    processed_items = []

    for item in data_batch:
        try:
            processed_item = {
                'id': item.get('id_sanpham', ''),
                'platform': item.get('platform', ''),
                'engagement_score': calculate_engagement_score(item),
                'processed_at': datetime.now().isoformat()
            }
            processed_items.append(processed_item)
        except Exception as e:
            continue

    return processed_items


def calculate_engagement_score(item):
    """Calculate engagement score for item"""
    try:
        likes = safe_int_convert(item.get('like', 0))
        comments = safe_int_convert(item.get('comment', 0))
        shares = safe_int_convert(item.get('share', 0))

        return likes + comments * 5 + shares * 10
    except:
        return 0


# ==================== CACHE MANAGEMENT ====================

def clear_data_cache():
    """Clear all data-related caches"""
    st.cache_data.clear()
    st.success("‚úÖ ƒê√£ x√≥a cache d·ªØ li·ªáu!")


def get_cache_stats():
    """Get cache statistics"""
    return {
        'cache_hits': 'Not available in current Streamlit version',
        'cache_size': 'Not available in current Streamlit version',
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }


# ==================== HEALTH CHECK ====================

@st.cache_data(ttl=300)  # Cache 5 ph√∫t
def health_check():
    """Health check cho data processor"""
    try:
        # Test connection
        connection_ok = connect_to_milvus()

        # Test collection
        collection_ok = check_collection_exists("product_collection_v4")
        
        # Get collection info n·∫øu c√≥ th·ªÉ
        collection_info = get_collection_info()

        return {
            'status': 'healthy' if connection_ok and collection_ok else 'degraded',
            'connection': connection_ok,
            'collection': collection_ok,
            'collection_info': collection_info,
            'timestamp': datetime.now().isoformat()
        }
    except Exception as e:
        return {
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
